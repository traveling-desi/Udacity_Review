{\rtf1\ansi\ansicpg1252\cocoartf1504
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 Menlo-Regular;
\f3\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red36\green36\blue36;\red255\green255\blue255;\red255\green255\blue255;
\red0\green0\blue0;\red21\green163\blue221;\red70\green81\blue90;\red35\green46\blue57;\red151\green0\blue126;
\red242\green242\blue242;\red38\green38\blue38;\red240\green240\blue240;\red0\green0\blue0;\red27\green29\blue31;
\red77\green77\blue77;\red249\green250\blue251;}
{\*\expandedcolortbl;\csgray\c100000;\cssrgb\c18824\c18824\c18824;\cssrgb\c100000\c100000\c100000;\csgray\c100000;
\cssrgb\c0\c0\c0;\cssrgb\c784\c70196\c89412;\cssrgb\c34510\c39216\c42745;\cssrgb\c18039\c23922\c28627;\cssrgb\c66667\c5098\c56863;
\cssrgb\c96078\c96078\c96078;\cssrgb\c20000\c20000\c20000;\cssrgb\c95294\c95294\c95294;\csgenericrgb\c0\c0\c0;\cssrgb\c14118\c15294\c16078;
\csgray\c37475;\cssrgb\c98039\c98431\c98824;}
\margl1440\margr1440\vieww20460\viewh11040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\pard\pardeftab720\sl640\sa230\partightenfactor0

\fs72 \cf2 \cb3 \expnd0\expndtw0\kerning0
Meets Specifications
\fs40 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \cb4 \kerning1\expnd0\expndtw0 \
\
\pard\pardeftab720\sl400\sa300\partightenfactor0

\fs28 \cf5 \expnd0\expndtw0\kerning0
You are on the forefront of the cutting edge research in machine translation. This was a difficult project and had many challenges, not the least of which was how everything was so new. But you have done a great job.\
\
Here are some additional resources that you may find helpful:\
\pard\pardeftab720\sl400\partightenfactor0
\cf6 https://www.oreilly.com/learning/capturing-semantic-meanings-using-deep-learning\
\pard\pardeftab720\sl400\partightenfactor0
\cf5 \uc0\u8232 \cf6 http://distill.pub/2016/augmented-rnns/\cf5 \uc0\u8232 \cf6 \
https://arxiv.org/pdf/1409.3215v3.pdf\cf5 \uc0\u8232 \cf6 \
https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\cf5 \uc0\u8232 \cf6 \
https://opensource.googleblog.com/2017/04/tf-seq2seq-sequence-to-sequence-framework-in-tensorflow.html\cf5 \uc0\u8232 \cf6 \
https://arxiv.org/pdf/1703.01619.pdf\cf5 \uc0\u8232 \cf6 \
http://karpathy.github.io/2015/05/21/rnn-effectiveness/\cf5 \
\pard\pardeftab720\sl320\partightenfactor0
\cf7 \cb3 \
\
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \cb4 \
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf8 All the unit tests in project have passed.
\f1 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f0\b0\fs24 \cf5 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\fs28 \cf5 Good job. Unit tests are a good way to sanitize your code in chunks. This helps isolate problems to easier to code debug code chunks.\
Always a good practice to write/break your code into small functions to do something focused, granular and easily measured and write unit tests to check that the function. Here are some good attributes of a good function:\
```
\f1 \
\pard\pardeftab720\sl440\partightenfactor0

\f2 \cf9 \cb10 short\cf11  code \cf9 size\cf11 \
high efficiency\
low memory footprint\
high reliability\
high generality\
\pard\pardeftab720\sl320\partightenfactor0

\f0 \cf7 \cb3 \
```\
\pard\pardeftab720\sl560\sa230\partightenfactor0

\b\fs36 \cf8 \
\pard\pardeftab720\sl320\partightenfactor0

\b0\fs28 \cf7 \cb4 \
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf8 \cb3 The function\'a0\cf2 \cb12 text_to_ids\cf8 \cb3 \'a0is implemented correctly.\

\fs28 \cb4 \
\pard\pardeftab720\sl400\partightenfactor0

\b0 \cf7 \cb3 Well done using nested list comprehension\'a0\'a0\cb4 :raised_hands: :+1:\
\
\
\pard\pardeftab720\sl320\partightenfactor0

\b\fs36 \cf13 The function\'a0model_inputs\'a0is implemented correctly.
\b0\fs28 \cf7 \
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \
While input is not a keyword in Python it is a built_function and you should avoid naming variables as [input](https://stackoverflow.com/a/20670757)\
\
T
\f3\fs30 \cf14 \cb3 he convention is to place an underscore after it
\f0\fs28 \cf7 \cb4 \
\
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf8 \cb3 The function\'a0\cf2 \cb12 process_decoding_input\cf8 \cb3 \'a0is implemented correctly.\

\fs28 \cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\b0 \cf7 \cb3 Well done. Nifty use of Slicing, filling and concatenating the data.\cb4 \
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \cb3 You didn't resort to\'a0`
\f2 \cf2 \cb12 tf.while_loop()`
\f0 \cf7 \cb3 \'a0or other looping structures which would be an inefficient way to do this. Loops are the first instinct that people have when faced with problems like these and almost always wrong/inefficient.\cb4 \
\
\
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf8 The function\'a0\cf2 \cb12 encoding_layer\cf8 \cb4 \'a0is implemented correctly.
\f1 \
\pard\pardeftab720\sl400\partightenfactor0

\fs24 \cf8 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f0\b0\fs28 \cf15 Good job adding dropouts.\
\pard\pardeftab720\sl400\partightenfactor0
\cf15 Dropouts are a way to regularize so that the network does not overfit.\
\
\pard\pardeftab720\sl400\partightenfactor0
\cf5 \
\pard\pardeftab720\sl280\partightenfactor0

\f1\fs24 \cf5 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b\fs36 \cf8 The function\'a0\cf2 \cb12 decoding_layer_train\cf8 \cb4 \'a0is implemented correctly.
\f1 \
\pard\pardeftab720\sl400\partightenfactor0

\fs24 \cf8 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f0\b0\fs28 \cf7 \cb3 You forgot to add the dropouts here. Dropouts are a way to regularize so that the network does not overfit.\
It is usually a good idea to add dropouts in the training section.\
EDIT: I see that you have added the dropouts in the `decoding_layer` function.\
\
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf8 The function\'a0\cf2 \cb12 decoding_layer_infer\cf8 \cb3 \'a0is implemented correctly.\

\fs28 \cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\b0 \cf7 \cb3 Good job not adding dropouts here\'a0:smile:\cb4 \
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \cb3 Remember we want dropouts during training so that we add redundancy. However, while inferring (testing) we don't want to add dropouts.\cb4 \
\pard\pardeftab720\sl320\partightenfactor0
\cf7 \
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf7 \
\pard\pardeftab720\sl320\partightenfactor0
\cf7 \
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf13 The function\'a0decoding_layer\'a0is implemented correctly.\
\pard\pardeftab720\sl400\partightenfactor0

\fs28 \cf8 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\b0 \cf7 \cb3 Well done reusing the variables between the training and inference. You were still awake and paying attention to the instructions\'a0:smile:\cb4 \
\pard\pardeftab720\sl300\partightenfactor0

\f2\fs25\fsmilli12600 \cf2 \cb12 `with tf.variable_scope("decode", reuse=True):`\

\f0\fs28 \cf7 \cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf7 \cb3 We want the weights learned in the train section to be reused in the inference section. Reusing variables allows us to do this.\cb4 \
\pard\pardeftab720\sl320\partightenfactor0
\cf7 \cb16 The other way to reuse variables within the same scope is with: \
\pard\pardeftab720\sl320\partightenfactor0
\cf7 \cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f2\fs25\fsmilli12600 \cf2 \cb12 `tf.get_variable_scope().reuse_variables()`\
More [details]({\field{\*\fldinst{HYPERLINK "https://www.tensorflow.org/api_docs/python/tf/variable_scope"}}{\fldrslt https://www.tensorflow.org/api_docs/python/tf/variable_scope}}):\
![Screen Shot 2017-09-17 at 9.33.06 AM.png](https://udacity-reviews-uploads.s3.amazonaws.com/_attachments/44782/1505666020/Screen_Shot_2017-09-17_at_9.33.06_AM.png)
\f0\fs28 \cf7 \cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf7 \cb3 Also, you have added the dropouts here in this function. The same cell on which you have added dropouts\'a0`
\f2\fs25\fsmilli12600 \cf2 \cb12 dec_cell`
\f0\fs28 \cf7 \cb3 \'a0gets passed to the training section as well as the inference section. It's correct to get dropouts in the training section. However, it is not correct to have dropouts in the inference section. So why am I letting this slide? Because the keep_prob that is passed during the training/running the network is 1.0 to the inference section. In effect, the dropouts do not get used in the inference section. However, as it is coded it is confusing.\cb4 \
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \cb3 It would be more clear if you just removed the dropout from the `
\f2\fs25\fsmilli12600 \cf2 \cb12 decoding_layer`
\f0\fs28 \cf7 \cb3  function and added them to the `
\f2\fs25\fsmilli12600 \cf2 \cb12 decoding_layer_
\f0\fs28 \cf7 \cb3 train` function. Then it is clear that the dropouts get used during training and not during inference. Makes sense?\cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf7 \cb3 \
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf13 \cb4 The function\'a0seq2seq_model\'a0is implemented correctly.\
\pard\pardeftab720\sl400\partightenfactor0

\fs28 \cf8 \
\pard\pardeftab720\sl400\partightenfactor0

\b0 \cf7 \cb3 Much easier function to code after all the heavy lifting in the previous functions. First, the encoder layer and then the decoder layer.\cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf7 \cb3 \
\pard\pardeftab720\sl400\partightenfactor0

\b\fs36 \cf8 The parameters are set to reasonable numbers.\

\fs28 \cb4 \
\
\pard\pardeftab720\sl400\sa300\partightenfactor0

\b0 \cf13 \cb3 These are reasonable numbers. However, I want to bring your attention to this:\cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f2\fs25\fsmilli12600 \cf13 \cb12 `Roughly the number of unique words: 227`
\f0\fs28 \cb4 \
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf13 \cb3 You have defined the embedding size to be more than the number of unique words in the vocabulary. You are not making use of any space savings by using embeddings. You could as well have used one hot encoded vector here and the vector length would have been about 227 only. But more importantly when you have embeddings larger than the number of unique words your model may start learning noise and may start overfitting.\cb4 \
\cb3 So, please make that correction in the next submission. Good choices are powers of 2. So, 128 but if that does not work then perhaps 196 or 200 or 220 (which are not powers of 2, but they are the best choice under the constraints)\cb4 \
\pard\pardeftab720\sl400\partightenfactor0
\cf13 \cb3 Other than that everything looks good.\
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \
\
******\
\
\pard\pardeftab720\sl400\sa300\partightenfactor0
\cf7 \cb16 These are good choices for a first attempt. But a rnn_size of 50 will make the network very weak and unable to learn the context more than a few words deep. Please change these to 128, 256 or 512 to build a much more powerful network.\cb4 \
\cb16 If your computer can handle a batch_size of 1024 without running out of memory, its fine else you should reduce the batch size to 128 or 256.\cb4 \
\cb16 Further, embedding sizes of 25 are too weak. The network may not be able to learn complex concepts of sentence structure of the language. You should try to increase it to 128, 200 or 220 or so.\cb4 \
\cb16 Finally, training an RNN/LSTM for longer than required may cause it to overtrain. I think typical epochs are in the range of 5 to 10.\cb4 \
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \cb16 These are general guidelines for you to try, you will need to experiment to find the sweet spot.\cb4 \
\pard\pardeftab720\sl320\partightenfactor0
\cf7 \
\
**********\
\
A few things:\
* You have not used any dropouts anywhere. So the keep_probability is pretty meaningless in your notebook. Most of the notebooks with good results that I have seen have dropouts on all the layers.\
* Try to keep the batch size as a power of 2. So instead of 100 use 128. Some hardware/software are optimized for powers of 2.\
* Embeddings are just like diaries for each of the vocabulary word. They store the encoded relationships etc for each vocabulary word. The larger the embedding size the more information it can learn and more connections it can make between different vocabulary words. However large embedding sizes also take a large time to run.\
\
\
\
\
\pard\pardeftab720\sl560\sa230\partightenfactor0

\b\fs36 \cf8 \cb16 Language Translation\
\pard\pardeftab720\sl320\partightenfactor0

\b0\fs28 \cf7 \cb4 \
\
The correct translation we are looking for is:\
\
` French Words: il a vu un vieux camion jaune . <EOS>`\
\
\pard\pardeftab720\sl400\partightenfactor0
\cf7 \
}